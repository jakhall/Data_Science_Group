{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import max_norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "path = '../../link_to_data_for_foundataion_of_ds/5000vocab_joined_comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + '/training_data_mbti_one_comment.csv')\n",
    "test = pd.read_csv(path + '/testing_data_mbti_one_comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  type  average_word_length  stop_word_%  upper_word_% t1 t2 t3  \\\n",
      "0         893  INTP                 4.46         0.35          0.05  I  N  T   \n",
      "1        1425  INFP                 4.37         0.40          0.04  I  N  F   \n",
      "2        2791  ENTP                 4.55         0.40          0.05  E  N  T   \n",
      "3        1205  INFJ                 4.50         0.31          0.05  I  N  F   \n",
      "4        7588  INTJ                 4.26         0.37          0.04  I  N  T   \n",
      "\n",
      "  t4  abandon  ...   youth  youtube  youve  yr  yup  zen  zero  zodiac  \\\n",
      "0  P        0  ...       0        0      0   0    0    0     0       0   \n",
      "1  P        0  ...       0        0      0   0    0    0     0       0   \n",
      "2  P        0  ...       0        0      2   0    0    0     0       0   \n",
      "3  J        0  ...       0        0      0   0    0    0     0       0   \n",
      "4  J        0  ...       0        0      0   1    0    0     0       0   \n",
      "\n",
      "   zombie  zone  \n",
      "0       0     0  \n",
      "1       0     0  \n",
      "2       0     0  \n",
      "3       0     0  \n",
      "4       0     0  \n",
      "\n",
      "[5 rows x 5008 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_type = train['type']\n",
    "t1 = train['t1']\n",
    "t2 = train['t2']\n",
    "t3 = train['t3']\n",
    "t4 = train['t4']\n",
    "train = train.drop(['t1','t2', 't3', 't4', 'user', 'type', 'Unnamed: 0'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_type_test = test['type']\n",
    "t1t = test['t1']\n",
    "t2t = test['t2']\n",
    "t3t = test['t3']\n",
    "t4t = test['t4']\n",
    "test = test.drop(['t1','t2', 't3', 't4', 'user', 'type', 'Unnamed: 0'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6940, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "#print(test.shape)\n",
    "#print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_scaled = preprocessing.StandardScaler().fit(train)\n",
    "train_scaled = X_scaled.transform(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Xt_scaled = preprocessing.StandardScaler().fit(test)\n",
    "test_scaled = X_scaled.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.array([t1 == 'E']).astype(int).T\n",
    "t2 = np.array([t2 == 'N']).astype(int).T\n",
    "t3 = np.array([t3 == 'F']).astype(int).T\n",
    "t4 = np.array([t4 == 'P']).astype(int).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1t = np.array([t1t == 'E']).astype(int)\n",
    "t2t = np.array([t2t == 'N']).astype(int)\n",
    "t3t = np.array([t3t == 'F']).astype(int)\n",
    "t4t = np.array([t4t == 'P']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(neurons, input_dim, epochs, momentum):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim = input_dim, kernel_constraint=max_norm(2), activation = 'relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(neurons, kernel_constraint=max_norm(2),activation = 'relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(neurons, kernel_constraint=max_norm(2),activation = 'relu'))\n",
    "    model.add(Dropout(0.6))  \n",
    "\n",
    " #   model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    " #   rms = SGD(lr= learning_rate, decay= learning_rate / epochs, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 400\n",
    "validation = 0.2\n",
    "epochs = 50\n",
    "neurons = 75\n",
    "l_rate = 0.00005\n",
    "model_t1 = build_model(neurons, train.shape[1],  epochs, 0.85)\n",
    "\n",
    "model_t3 = build_model(neurons, train.shape[1],  epochs, 0.85)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1735, 5001)\n",
      "(1, 1735)\n"
     ]
    }
   ],
   "source": [
    "callbacks1 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(filepath='best_model_t1.h5', monitor='val_loss', save_best_only=True)]\n",
    "callbacks2 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(filepath='best_model_t2.h5', monitor='val_loss', save_best_only=True)]\n",
    "callbacks3 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(filepath='best_model_t3.h5', monitor='val_loss', save_best_only=True)]\n",
    "callbacks4 = [EarlyStopping(monitor='val_loss', patience=1),\n",
    "             ModelCheckpoint(filepath='best_model_t4.h5', monitor='val_loss', save_best_only=True)]\n",
    "print(test_scaled.shape)\n",
    "print(t1t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6940/6940 [==============================] - 3s 372us/step\n",
      "1735/1735 [==============================] - 1s 337us/step\n",
      "[0.5731053141077245, 0.7694524495677233] [0.5997471477181492, 0.7700288184781583]\n"
     ]
    }
   ],
   "source": [
    "trained_t1 = model_t1.fit(x = train_scaled, y = t1, callbacks=callbacks1,epochs = epochs, batch_size = 200, verbose = False,  validation_split= validation)\n",
    "eval_t1 = model_t1.evaluate(x = train_scaled, y = t1)\n",
    "eval_t1t = model_t1.evaluate(x = test_scaled, y = t1t.T)\n",
    "print(eval_t1, eval_t1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5552 samples, validate on 1388 samples\n",
      "Epoch 1/50\n",
      "5552/5552 [==============================] - 8s 1ms/step - loss: 0.5283 - acc: 0.7821 - val_loss: 0.3912 - val_acc: 0.8718\n",
      "Epoch 2/50\n",
      "5552/5552 [==============================] - 1s 252us/step - loss: 0.4252 - acc: 0.8552 - val_loss: 0.4053 - val_acc: 0.8718\n",
      "6940/6940 [==============================] - 2s 336us/step\n",
      "1735/1735 [==============================] - 1s 351us/step\n",
      "[0.3942062158577724, 0.8609510086111789] [0.4135435838177156, 0.8662824207492795]\n"
     ]
    }
   ],
   "source": [
    "model_t2 = build_model(neurons, train.shape[1],  epochs, 0.85)\n",
    "trained_t2 = model_t2.fit(x = train_scaled, y = t2, callbacks=callbacks2,epochs = epochs, batch_size = 200,  validation_split= validation)\n",
    "eval_t2 = model_t2.evaluate(x = train_scaled, y = t2)\n",
    "eval_t2t = model_t2.evaluate(x = test_scaled, y = t2t.T)\n",
    "print(eval_t2, eval_t2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5552 samples, validate on 1388 samples\n",
      "Epoch 1/50\n",
      "5552/5552 [==============================] - 8s 1ms/step - loss: 0.7417 - acc: 0.5184 - val_loss: 0.6727 - val_acc: 0.5944\n",
      "Epoch 2/50\n",
      "5552/5552 [==============================] - 1s 266us/step - loss: 0.6948 - acc: 0.5596 - val_loss: 0.6525 - val_acc: 0.6571\n",
      "Epoch 3/50\n",
      "5552/5552 [==============================] - 1s 259us/step - loss: 0.6322 - acc: 0.6286 - val_loss: 0.5941 - val_acc: 0.7313\n",
      "Epoch 4/50\n",
      "5552/5552 [==============================] - 1s 259us/step - loss: 0.5615 - acc: 0.7064 - val_loss: 0.5165 - val_acc: 0.7594\n",
      "Epoch 5/50\n",
      "5552/5552 [==============================] - 1s 255us/step - loss: 0.4617 - acc: 0.7810 - val_loss: 0.4793 - val_acc: 0.7716\n",
      "Epoch 6/50\n",
      "5552/5552 [==============================] - 1s 254us/step - loss: 0.3852 - acc: 0.8285 - val_loss: 0.4785 - val_acc: 0.7781\n",
      "Epoch 7/50\n",
      "5552/5552 [==============================] - 1s 257us/step - loss: 0.2948 - acc: 0.8782 - val_loss: 0.5205 - val_acc: 0.7803\n",
      "6940/6940 [==============================] - 2s 338us/step\n",
      "1735/1735 [==============================] - 1s 366us/step\n",
      "[0.21547009845288412, 0.9236311238505999] [0.4874519396584041, 0.7867435159188526]\n"
     ]
    }
   ],
   "source": [
    "trained_t3 = model_t3.fit(x = train_scaled, y = t3, callbacks=callbacks3,epochs = epochs, batch_size = 200,  validation_split= validation)\n",
    "eval_t3 = model_t3.evaluate(x = train_scaled, y = t3)\n",
    "eval_t3t = model_t3.evaluate(x = test_scaled, y = t3t.T)\n",
    "print(eval_t3, eval_t3t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5552 samples, validate on 1388 samples\n",
      "Epoch 1/50\n",
      "5552/5552 [==============================] - 8s 1ms/step - loss: 0.7131 - acc: 0.5600 - val_loss: 0.6780 - val_acc: 0.6081\n",
      "Epoch 2/50\n",
      "5552/5552 [==============================] - 1s 264us/step - loss: 0.6884 - acc: 0.5740 - val_loss: 0.6756 - val_acc: 0.6059\n",
      "Epoch 3/50\n",
      "5552/5552 [==============================] - 1s 257us/step - loss: 0.6670 - acc: 0.5947 - val_loss: 0.6730 - val_acc: 0.6073\n",
      "Epoch 4/50\n",
      "5552/5552 [==============================] - 1s 262us/step - loss: 0.6511 - acc: 0.6124 - val_loss: 0.6651 - val_acc: 0.6182\n",
      "Epoch 5/50\n",
      "5552/5552 [==============================] - 1s 268us/step - loss: 0.6259 - acc: 0.6284 - val_loss: 0.6557 - val_acc: 0.6362\n",
      "Epoch 6/50\n",
      "5552/5552 [==============================] - 1s 263us/step - loss: 0.5919 - acc: 0.6639 - val_loss: 0.6412 - val_acc: 0.6434\n",
      "Epoch 7/50\n",
      "5552/5552 [==============================] - 1s 263us/step - loss: 0.5311 - acc: 0.7228 - val_loss: 0.6394 - val_acc: 0.6405\n",
      "Epoch 8/50\n",
      "5552/5552 [==============================] - 2s 282us/step - loss: 0.4691 - acc: 0.7756 - val_loss: 0.6782 - val_acc: 0.6326\n",
      "6940/6940 [==============================] - 3s 375us/step\n",
      "1735/1735 [==============================] - 1s 385us/step\n",
      "[0.36028910682936566, 0.8772334293261042] [0.6552030039795538, 0.6489913545183902]\n"
     ]
    }
   ],
   "source": [
    "model_t4 = build_model(neurons, train.shape[1],  epochs, 0.85)\n",
    "trained_t4 = model_t4.fit(x = train_scaled, y = t4,callbacks=callbacks4, epochs = epochs, batch_size = 200,  validation_split= validation)\n",
    "eval_t4 = model_t4.evaluate(x = train_scaled, y = t4)\n",
    "eval_t4t = model_t4.evaluate(x = test_scaled, y = t4t.T)\n",
    "print(eval_t4, eval_t4t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7700288184781583 0.8662824207492795 0.7867435159188526 0.6489913545183902\n"
     ]
    }
   ],
   "source": [
    "print(eval_t1t[1], eval_t2t[1], eval_t3t[1], eval_t4t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28091612]\n",
      " [0.2823671 ]\n",
      " [0.34586498]\n",
      " [0.2656325 ]\n",
      " [0.24388891]\n",
      " [0.30145082]\n",
      " [0.2798758 ]\n",
      " [0.3300843 ]\n",
      " [0.3149894 ]\n",
      " [0.35464972]\n",
      " [0.29348773]\n",
      " [0.27654028]\n",
      " [0.29671323]\n",
      " [0.29648384]\n",
      " [0.336347  ]\n",
      " [0.27972227]\n",
      " [0.389725  ]\n",
      " [0.35235772]\n",
      " [0.43924776]\n",
      " [0.31885964]\n",
      " [0.3020974 ]\n",
      " [0.28448126]\n",
      " [0.32630682]\n",
      " [0.29284257]]\n"
     ]
    }
   ],
   "source": [
    "print(pred[1:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f94d6ebf842d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "np.sum(a) / (np.sum(b) + np.sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "rft1 = RandomForestClassifier()\n",
    "rft2 = RandomForestClassifier()\n",
    "rft3 = RandomForestClassifier()\n",
    "rft4 = RandomForestClassifier()\n",
    "rft5 = RandomForestClassifier()\n",
    "\n",
    "#rft1 = LogisticRegression(C=0.005)\n",
    "#rft2 = LogisticRegression(C=0.005)\n",
    "#rft3 = LogisticRegression(C=0.005)\n",
    "#rft4 = LogisticRegression(C=0.005)\n",
    "#rft5 = LogisticRegression(C=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft1.fit(train_scaled, t1)\n",
    "rft2.fit(train_scaled, t2)\n",
    "rft3.fit(train_scaled, t3)\n",
    "rft4.fit(train_scaled, t4)\n",
    "rft5.fit(train_scaled, f_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910662824207492 0.9979827089337175 0.9926512968299712 0.9762247838616714 0.996685878962536\n"
     ]
    }
   ],
   "source": [
    "scoret1 = rft1.score(train_scaled, t1)\n",
    "scoret2 = rft2.score(train_scaled, t2)\n",
    "scoret3 = rft3.score(train_scaled, t3)\n",
    "scoret4 = rft4.score(train_scaled, t4)\n",
    "scoret5 = rft5.score(train_scaled, f_type)\n",
    "print(scoret1, scoret2, scoret3, scoret4, scoret5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729106628242075 0.8109510086455332 0.7694524495677233 0.6305475504322766 0.33602305475504324\n"
     ]
    }
   ],
   "source": [
    "scoret1 = rft1.score(test_scaled, t1t.T)\n",
    "scoret2 = rft2.score(test_scaled, t2t.T)\n",
    "scoret3 = rft3.score(test_scaled, t3t.T)\n",
    "scoret4 = rft4.score(test_scaled, t4t.T)\n",
    "scoret5 = rft5.score(test_scaled, f_type_test)\n",
    "print(scoret1, scoret2, scoret3, scoret4, scoret5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rft1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-326-e6ffc3d70d63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrft1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrft2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m550\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrft3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrft4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoret1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoret2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoret3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoret4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoret5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rft1' is not defined"
     ]
    }
   ],
   "source": [
    "print(rft1.predict(test_scaled[1:250]))\n",
    "print(rft2.predict(test_scaled[1:550]))\n",
    "print(rft3.predict(test_scaled[1:250]))\n",
    "print(rft4.predict(test_scaled[1:250]))\n",
    "print(scoret1, scoret2, scoret3, scoret4, scoret5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23323548496740104"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
